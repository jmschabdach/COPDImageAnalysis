{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# handling different types of data\n",
    "import pandas as pd\n",
    "import pickle as pk\n",
    "import shelve\n",
    "import h5py\n",
    "from joblib import Parallel, delayed  # conda install -c anaconda joblib=0.9.4\n",
    "from cyflann import *\n",
    "import argparse\n",
    "import scipy.sparse as sp\n",
    "from scipy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadSubjectGraph(fn):\n",
    "    \"\"\"\n",
    "    Load the subject graphs from an HDF5 file.\n",
    "\n",
    "    Inputs:\n",
    "    - fn: filename to load from \n",
    "\n",
    "    Returns:\n",
    "    - graphs: loaded subject graphs\n",
    "    \"\"\"\n",
    "    print \"Loading the subject graph...\"\n",
    "    fn = fn + \".h5\"\n",
    "    with h5py.File(fn, 'r') as hf:\n",
    "        # print(\"List of arrays in this file: \\n\" + str(hf.keys()))\n",
    "        metadata = hf.get('metadata').shape\n",
    "        print metadata\n",
    "        graph = []\n",
    "        for j in xrange(metadata[0]):\n",
    "            # get the name of the group\n",
    "            dsName = str(j).zfill(4)\n",
    "            # extract the group and the items from the groups\n",
    "            g = hf.get(dsName)\n",
    "            nodes = g.get(\"nodes\")\n",
    "            dists = g.get(\"dists\")\n",
    "            # put the items into the data structure\n",
    "            temp = {\n",
    "                \"nodes\": np.array(nodes),\n",
    "                \"dists\": np.array(dists)\n",
    "            }\n",
    "            graph.append(temp)\n",
    "    print \"Graph loaded!\"\n",
    "    return graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadPickledData():     \n",
    "\n",
    "    # pickleFn =  '%(pickleRootFolder)s/%(pickleSettingName)s/%(pickleSettingName)s.data.p'%\\\n",
    "        # {'pickleSettingName':pickleSettingName, 'pickleRootFolder':pickleRootFolder}\n",
    "\n",
    "    # On Bridges for job\n",
    "    # pickleFn =  '/pylon1/ms4s88p/jms565/COPDGene_pickleFiles/histFHOG_largeRange_setting1.data.p'\n",
    "    # shelveFn = '/pylon1/ms4s88p/jms565/COPDGene_pickleFiles/histFHOG_largeRange_setting1.shelve'\n",
    "\n",
    "    # desktop or laptop or home\n",
    "    pickleFn = \"COPDGene_pickleFiles/histFHOG_largeRange_setting1.data.p\"\n",
    "    shelveFn = 'COPDGene_pickleFiles/histFHOG_largeRange_setting1.shelve'\n",
    "    print \"pickleFn : \", pickleFn\n",
    "    print \"shelveFn :\", shelveFn\n",
    "    \n",
    "    # reading pickle and shelve files\n",
    "    print \"Reading the shelve file ...\"\n",
    "    fid = shelve.open(shelveFn,'r')\n",
    "    metaVoxelDict = fid['metaVoxelDict']\n",
    "    subjList = fid['subjList']\n",
    "    phenotypeDB_clean = fid['phenotypeDB_clean']\n",
    "    fid.close()\n",
    "    print \"Done !\"\n",
    "    print \"Sample of the metadata: \"\n",
    "    print \"IDs of a few subjects : \" , metaVoxelDict[0]['id']\n",
    "    print \"labelIndex of the meta data (a few elements): \" , metaVoxelDict[0]['labelIndex'][1:10]   \n",
    "    print \"Reading pickle file ....\"\n",
    "    fid = open(pickleFn,'rb')\n",
    "    data = pk.load(open(pickleFn,'rb'))\n",
    "    fid.close()\n",
    "    print \"Done !\"\n",
    "    return metaVoxelDict,subjList, phenotypeDB_clean, data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the subject graph...\n",
      "(7292,)\n",
      "Graph loaded!\n"
     ]
    }
   ],
   "source": [
    "fn = \"0000\"\n",
    "subjGraph = loadSubjectGraph(\"individualSubjectGraphs/\"+fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickleFn :  COPDGene_pickleFiles/histFHOG_largeRange_setting1.data.p\n",
      "shelveFn : COPDGene_pickleFiles/histFHOG_largeRange_setting1.shelve\n",
      "Reading the shelve file ...\n",
      "Done !\n",
      "Sample of the metadata: \n",
      "IDs of a few subjects :  10002K\n",
      "labelIndex of the meta data (a few elements):  [132 133 154 163 170 180 181 182 184]\n",
      "Reading pickle file ....\n",
      "Done !\n"
     ]
    }
   ],
   "source": [
    "# read data from original files\n",
    "metaVoxelDict, subjList, phenotypeDB_clean, data = loadPickledData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7292\n",
      "1573110\n",
      "[    0.   180.   442.   673.   877.  1144.  1352.  1549.  1822.  2082.]\n",
      "[  179.   441.   672.   876.  1143.  1351.  1548.  1821.  2081.  2289.]\n",
      "[180, 262, 231, 204, 267, 208, 197, 273, 260, 208]\n"
     ]
    }
   ],
   "source": [
    "# look at size of each file\n",
    "numSubjSuperPixels = [ len(s['I']) for s in data ]\n",
    "print len(numSubjSuperPixels)\n",
    "totalSuperPixels = sum(numSubjSuperPixels)\n",
    "print totalSuperPixels\n",
    "# create list/dictionary for storing sizes of each subject, start index, and end index?\n",
    "superPixelIndexingStart = np.zeros(len(numSubjSuperPixels))\n",
    "superPixelIndexingEnd = np.zeros(len(numSubjSuperPixels))\n",
    "# subj1: 0 - len(subj1)-\n",
    "# subj2: len(subj1) - len(subj1)+len(subj2)-1\n",
    "for i in xrange(len(numSubjSuperPixels)):\n",
    "    if i == 0 :\n",
    "        superPixelIndexingStart[i] = 0\n",
    "        superPixelIndexingEnd[i] = numSubjSuperPixels[i]-1\n",
    "    else:\n",
    "        superPixelIndexingStart[i] = numSubjSuperPixels[i-1] + superPixelIndexingStart[i-1]\n",
    "        superPixelIndexingEnd[i] = numSubjSuperPixels[i] + superPixelIndexingEnd[i-1]\n",
    "\n",
    "# return the list/dictionary\n",
    "superMetaData = {\n",
    "    \"totalSuperPixels\": totalSuperPixels,\n",
    "    \"subjectSuperPixels\": numSubjSuperPixels,\n",
    "    # add both start and end and figure out which one to use later\n",
    "    \"superPixelIndexingStart\": superPixelIndexingStart,\n",
    "    \"superPixelIndexingEnd\": superPixelIndexingEnd\n",
    "}\n",
    "\n",
    "print superPixelIndexingStart[0:10]\n",
    "print superPixelIndexingEnd[0:10]\n",
    "print numSubjSuperPixels[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished building this graph\n",
      "(1573110, 180)\n"
     ]
    }
   ],
   "source": [
    "# Uses the superMetaData, subjIdx, numSimNodes\n",
    "subjIdx = 0\n",
    "numSimNodes = 5\n",
    "# set up matrix: number of elements in DB subject x total number of elements in query subjects\n",
    "numSubjPix = superMetaData[\"subjectSuperPixels\"][subjIdx]\n",
    "\n",
    "rows = np.matrix([[i] * numSimNodes for i in xrange(superMetaData[\"subjectSuperPixels\"][0])])\n",
    "# set up initial sparse matrix\n",
    "subjJShape = (superMetaData[\"subjectSuperPixels\"][0], numSubjPix)\n",
    "# get 3 closest distances \n",
    "cols = subjGraph[0][\"nodes\"][:, 0:numSimNodes] \n",
    "dists = subjGraph[0][\"dists\"][:, 0:numSimNodes]\n",
    "# make sparse matrix here\n",
    "sparseSubj = sp.csr_matrix( (list(dists.flat),(list(rows.flat), list(cols.flat))), shape=subjJShape)\n",
    "\n",
    "# for each query subject in the h5 file for one db subject\n",
    "for j in xrange(len(subjGraph)-1):\n",
    "    subjJShape = (superMetaData[\"subjectSuperPixels\"][j+1], numSubjPix)\n",
    "    # get 3 closest distances \n",
    "    cols = subjGraph[j+1][\"nodes\"][:, 0:numSimNodes] \n",
    "    dists = subjGraph[j+1][\"dists\"][:, 0:numSimNodes]\n",
    "    rows = np.matrix([[i] * numSimNodes for i in xrange(superMetaData[\"subjectSuperPixels\"][j+1])])\n",
    "    # make sparse matrix here\n",
    "    sparseJ = sp.csr_matrix( (list(dists.flat),(list(rows.flat), list(cols.flat))), shape=subjJShape)\n",
    "    # concatenate w/ row matrix?\n",
    "    sparseSubj = sp.vstack((sparseSubj, sparseJ), format='csr')\n",
    "    \n",
    "print \"Finished building this graph\"\n",
    "A = sparseSubj.todense()\n",
    "print A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1573110, 180)\n",
      "(201, 180)\n"
     ]
    }
   ],
   "source": [
    "A = sparseSubj.todense()\n",
    "print A.shape\n",
    "B = sparseJ.todense()\n",
    "print B.shape\n",
    "subjI = sp.vstack((sparseSubj, sparseJ), format='csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compileGraphSingleSubj(subjGraph, superMetaData, subjIdx, numSimNodes=5):\n",
    "    \"\"\" \n",
    "    Extract the data from a single subject into a massive matrix\n",
    "    Matrix size is # elements in DB subject x # elements in query subject\n",
    "\n",
    "    Inputs:\n",
    "    - subjGraph: data loaded from h5 files\n",
    "    - superMetaData: data about the size and index of the query subjects\n",
    "                    (from getSubjectSizes())\n",
    "    - subjIdx: number for identifying the current subject\n",
    "    - numSimNodes (opt): how many similar nodes will be placed in the matrix\n",
    "\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    # Uses the superMetaData, subjIdx, numSimNodes\n",
    "    numSubjPix = superMetaData[\"subjectSuperPixels\"][subjIdx]\n",
    "\n",
    "    # set up initial sparse matrix\n",
    "    subjJShape = (superMetaData[\"subjectSuperPixels\"][0], numSubjPix)\n",
    "    # get 3 closest distances for all elements in subj\n",
    "    cols = subjGraph[0][\"nodes\"][:, 0:numSimNodes] \n",
    "    dists = subjGraph[0][\"dists\"][:, 0:numSimNodes]\n",
    "    rows = np.matrix([[i] * numSimNodes for i in xrange(superMetaData[\"subjectSuperPixels\"][0])])\n",
    "    # make sparse matrix here\n",
    "    sparseSubj = sp.csr_matrix( (list(dists.flat),(list(rows.flat), list(cols.flat))), shape=subjJShape)\n",
    "\n",
    "    # for each query subject in the h5 file for one db subject\n",
    "    for j in xrange(len(subjGraph)-1):\n",
    "        subjJShape = (superMetaData[\"subjectSuperPixels\"][j+1], numSubjPix)\n",
    "        # get 3 closest distances \n",
    "        cols = subjGraph[j+1][\"nodes\"][:, 0:numSimNodes] \n",
    "        dists = subjGraph[j+1][\"dists\"][:, 0:numSimNodes]\n",
    "        rows = np.matrix([[i] * numSimNodes for i in xrange(superMetaData[\"subjectSuperPixels\"][j+1])])\n",
    "        # make sparse matrix here\n",
    "        sparseJ = sp.csr_matrix( (list(dists.flat),(list(rows.flat), list(cols.flat))), shape=subjJShape)\n",
    "        # concatenate w/ row matrix?\n",
    "        sparseSubj = sp.vstack((sparseSubj, sparseJ), format='csr')\n",
    "        \n",
    "    print \"Finished building single graph for DB subject \" + str(subjIdx) + \"!\"\n",
    "    return sparseSubj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the subject graph...\n",
      "(7292,)\n",
      "Graph loaded!\n",
      "Finished building single graph for DB subject 0!\n",
      "Loading the subject graph...\n",
      "(7292,)\n",
      "Graph loaded!\n",
      "Finished building single graph for DB subject 1!\n",
      "Loading the subject graph...\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "Unable to open file (Unable to open file: name = 'individualsubjectgraphs/0002.h5', errno = 2, error message = 'no such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-041a1460e091>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msuperMetaData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"subjectSuperPixels\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mfn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0msubjGraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloadSubjectGraph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"individualSubjectGraphs/\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[1;31m# compileGraphSingleSubj()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0msparseSubjI\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompileGraphSingleSubj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubjGraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuperMetaData\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumSimNodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-063e30c624cb>\u001b[0m in \u001b[0;36mloadSubjectGraph\u001b[1;34m(fn)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Loading the subject graph...\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mfn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".h5\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[1;31m# print(\"List of arrays in this file: \\n\" + str(hf.keys()))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mmetadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'metadata'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jenna/anaconda2/lib/python2.7/site-packages/h5py/_hl/files.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 272\u001b[1;33m                 \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jenna/anaconda2/lib/python2.7/site-packages/h5py/_hl/files.pyc\u001b[0m in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m             \u001b[0mflags\u001b[0m \u001b[1;33m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'r+'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (-------src-dir-------/h5py/_objects.c:2582)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (-------src-dir-------/h5py/_objects.c:2541)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open (-------src-dir-------/h5py/h5f.c:1816)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: Unable to open file (Unable to open file: name = 'individualsubjectgraphs/0002.h5', errno = 2, error message = 'no such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "# set up initial graph:\n",
    "fn = str(0).zfill(4)\n",
    "subjGraph = loadSubjectGraph(\"individualSubjectGraphs/\"+fn)\n",
    "# compileGraphSingleSubj()\n",
    "sparseGraph = compileGraphSingleSubj(subjGraph, superMetaData, 0, numSimNodes=3)\n",
    "\n",
    "# for each subject\n",
    "for s in xrange(len(superMetaData[\"subjectSuperPixels\"])-1):\n",
    "    fn = str(s+1).zfill(4)\n",
    "    subjGraph = loadSubjectGraph(\"individualSubjectGraphs/\"+fn)\n",
    "    # compileGraphSingleSubj()\n",
    "    sparseSubjI = compileGraphSingleSubj(subjGraph, superMetaData, s+1, numSimNodes=3)\n",
    "    sparseGraph = sp.hstack((sparseGraph, sparseSubjI), format='csr')\n",
    "# return the massive joint graph matrix\n",
    "print \"Finished compiling complete sparse graph!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1573110, 442)\n",
      "442\n",
      "<class 'numpy.matrixlib.defmatrix.matrix'>\n"
     ]
    }
   ],
   "source": [
    "D = sparseGraph.todense()\n",
    "print D.shape\n",
    "print str(sum(superMetaData[\"subjectSuperPixels\"][0:2]))\n",
    "print type(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the subject graph...\n",
      "(7292,)\n",
      "Graph loaded!\n",
      "Finished building this graph!\n"
     ]
    }
   ],
   "source": [
    "fn = \"0000\"\n",
    "subjGraph = loadSubjectGraph(\"individualSubjectGraphs/\"+fn)\n",
    "\n",
    "# Uses the superMetaData, subjIdx, numSimNodes\n",
    "subjIdx = 0\n",
    "numSimNodes = 5\n",
    "# set up matrix: number of elements in DB subject x total number of elements in query subjects\n",
    "numSubjPix = superMetaData[\"subjectSuperPixels\"][subjIdx]\n",
    "singleSubjGraph = np.zeros((superMetaData[\"totalSuperPixels\"], numSubjPix))\n",
    "# for each query subject in the h5 file\n",
    "for j in xrange(len(superMetaData[\"subjectSuperPixels\"])):\n",
    "    for k in xrange(len(subjGraph[j][\"nodes\"])):\n",
    "        # get 3 closest distances \n",
    "        nodes = subjGraph[j][\"nodes\"][k][0:numSimNodes]\n",
    "        dists = subjGraph[j][\"dists\"][k][0:numSimNodes]\n",
    "        # put dists at the location (db subj node, query subj nodes)\n",
    "        shiftedK = int(superMetaData[\"superPixelIndexingStart\"][j]+k)\n",
    "        for i in xrange(numSimNodes):\n",
    "            singleSubjGraph[shiftedK][nodes[i]] = dists[i]\n",
    "# * make sure to adjust the query subj nodes wrt the offset from the prev subjs\n",
    "print \"Finished building this graph!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1573110, 180)\n",
      "(1573110, 180)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print singleSubjGraph.shape\n",
    "print A.shape\n",
    "print np.asarray(A).all()==singleSubjGraph.all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved the files\n",
      "Sparse graph loaded\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def saveSparseGraph(graph, fn):\n",
    "    \"\"\"\n",
    "    Try to save the graph using numpy.save\n",
    "    \n",
    "    Inputs:\n",
    "    - graph: the csr_matrix to save\n",
    "    - fn: the filename base (no extensions)\n",
    "    \n",
    "    Returns: none\n",
    "    \"\"\"\n",
    "    np.savez(fn, data=graph.data, indices=graph.indices, indptr=graph.indptr, shape=graph.shape)\n",
    "    print \"Saved the files\"\n",
    "    \n",
    "    \n",
    "def loadSparseGraph(fn):\n",
    "    \"\"\"\n",
    "    Try to load the previously saved graph\n",
    "    \n",
    "    Inputs:\n",
    "    - fn: the file name/path base (no extensions)\n",
    "    \n",
    "    Returns: \n",
    "    - the loaded sparse matrix\n",
    "    \"\"\"\n",
    "    loader = np.load(fn+\".npz\")\n",
    "    print \"Sparse graph loaded\"\n",
    "    return sp.csr_matrix((loader['data'], loader['indices'], loader['indptr']), shape=loader['shape'])\n",
    "\n",
    "\n",
    "fn = \"./graph-test\"\n",
    "saveSparseGraph(sparseGraph, fn)\n",
    "loadedSparseGraph = loadSparseGraph(fn)\n",
    "\n",
    "E = loadedSparseGraph.todense()\n",
    "print D.all()==E.all()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
